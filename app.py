import streamlit as st
import os
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

from src.models.llm import get_llm
from src.services.vector_store import VectorStoreService
from src.utils.document_loader import DocumentLoader
from src.agents.router_graph import AgentRouterGraph  # â† Thay Ä‘á»•i import

load_dotenv()

st.set_page_config(
    page_title="AI Workshop - Medical Assistant",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS Ä‘á»ƒ lÃ m Ä‘áº¹p UI
st.markdown("""
<style>
    .main-header {
        font-size: 2rem;
        font-weight: bold;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        padding: 0.5rem 0;
    }
    .progress-section {
        font-size: 0.85rem;
        margin: 0.5rem 0;
    }
    .progress-section h3 {
        font-size: 1rem;
        margin-bottom: 0.5rem;
    }
    .info-box {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
        color: #1a1a1a;
        font-weight: 500;
    }
    .info-box.medical {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
    }
    .info-box.doctor {
        background-color: #f3e5f5;
        border-left: 4px solid #9c27b0;
    }
    .info-box.medicine {
        background-color: #fff3e0;
        border-left: 4px solid #ff9800;
    }
    .warning-box {
        background-color: #fff9c4;
        border: 2px solid #f57c00;
        border-radius: 0.5rem;
        padding: 1rem;
        margin: 1rem 0;
        color: #e65100;
        font-weight: 600;
    }
    .warning-box strong {
        color: #bf360c;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_resource
def init_chatbot():
    """Khá»Ÿi táº¡o chatbot"""
    try:
        use_unstructured = os.getenv('USE_UNSTRUCTURED', 'false').lower() == 'true'
        
        llm = get_llm(streaming=True)
        vector_service = VectorStoreService()
        document_loader = DocumentLoader(use_unstructured=use_unstructured)
        router = AgentRouterGraph(vector_service=vector_service)  # â† LangGraph Router
        
        chat_history = ChatMessageHistory()
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", "Báº¡n lÃ  trá»£ lÃ½ y táº¿ AI thÃ´ng minh, thÃ¢n thiá»‡n vÃ  há»¯u Ã­ch."),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])
        
        chain = prompt | llm
        conversation = RunnableWithMessageHistory(
            chain,
            lambda session_id: chat_history,
            input_messages_key="input",
            history_messages_key="history"
        )
        
        try:
            vector_service.load_vector_store()
        except:
            folder_path = "./data/documents"
            if os.path.exists(folder_path):
                documents = document_loader.load_documents_from_folder(folder_path)
                if documents:
                    vector_service.create_vector_store(documents)
            
        return llm, vector_service, document_loader, router, chat_history, conversation
    except Exception as e:
        st.error(f"âŒ Lá»—i khá»Ÿi táº¡o: {str(e)}")
        return None, None, None, None, None, None

def build_conversation_context(chat_history) -> str:
    """Táº¡o context tá»« lá»‹ch sá»­ há»™i thoáº¡i"""
    if not chat_history or not chat_history.messages:
        return ""
    
    recent_messages = chat_history.messages[-6:]
    context_parts = []
    for msg in recent_messages:
        role = "Bá»‡nh nhÃ¢n" if msg.type == "human" else "BÃ¡c sÄ©"
        context_parts.append(f"{role}: {msg.content}")
    
    return "\n".join(context_parts)

def load_documents():
    """Táº£i tÃ i liá»‡u tá»« thÆ° má»¥c"""
    folder_path = "./data/documents"
    
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        return []
        
    documents = st.session_state.document_loader.load_documents_from_folder(folder_path)
    
    if documents:
        st.session_state.vector_service.create_vector_store(documents)
        return documents
    return []

def get_intent_icon_and_color(intent: str):
    """Láº¥y icon vÃ  mÃ u theo intent"""
    intent_map = {
        "medical_consultation": ("ğŸ¥", "medical", "TÆ° váº¥n y táº¿"),
        "doctor_recommendation": ("ğŸ‘¨â€âš•ï¸", "doctor", "Gá»£i Ã½ bÃ¡c sÄ©"),
        "medicine_inquiry": ("ğŸ’Š", "medicine", "TÆ° váº¥n thuá»‘c"),
        "general_chat": ("ğŸ’¬", "general", "TrÃ² chuyá»‡n")
    }
    return intent_map.get(intent, ("ğŸ’¬", "general", "TrÃ² chuyá»‡n"))

def display_conversation_step():
    """Hiá»ƒn thá»‹ bÆ°á»›c há»™i thoáº¡i hiá»‡n táº¡i"""
    num_messages = len(st.session_state.messages)
    
    if num_messages == 0:
        current_step = 0
    elif num_messages <= 2:
        current_step = 1
    elif num_messages <= 4:
        current_step = 2
    else:
        current_step = 3
    
    steps = ["ğŸ Báº¯t Ä‘áº§u", "ğŸ’¬ TÆ° váº¥n", "ğŸ‘¨â€âš•ï¸ Gá»£i Ã½", "ğŸ’Š Äiá»u trá»‹"]
    
    cols = st.columns(4)
    for i, step_name in enumerate(steps):
        with cols[i]:
            if i == current_step:
                st.write(f"**{step_name}**")
                st.progress(1.0)
            elif i < current_step:
                st.write(f"âœ… {step_name}")
                st.progress(1.0)
            else:
                st.write(f"âšª {step_name}")
                st.progress(0.0)

def main():
    st.markdown('<h1 class="main-header">ğŸ¥ AI Medical Assistant</h1>', unsafe_allow_html=True)
    
    # Initialize session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "user_messages_only" not in st.session_state:
        st.session_state.user_messages_only = []
    
    if "chatbot_initialized" not in st.session_state:
        with st.spinner("ğŸš€ Äang khá»Ÿi táº¡o AI Medical Assistant..."):
            llm, vector_service, document_loader, router, chat_history, conversation = init_chatbot()
            if llm:
                st.session_state.llm = llm
                st.session_state.vector_service = vector_service
                st.session_state.document_loader = document_loader
                st.session_state.router = router
                st.session_state.chat_history = chat_history
                st.session_state.conversation = conversation
                st.session_state.chatbot_initialized = True
            else:
                st.stop()
    
    # Sidebar
    with st.sidebar:
        if st.button("ğŸ—‘ï¸ XÃ³a lá»‹ch sá»­ chat", use_container_width=True, type="primary"):
            st.session_state.messages = []
            st.session_state.chat_history.clear()
            st.session_state.user_messages_only = []
            st.rerun()
        
        st.markdown("---")
        st.header("âš™ï¸ CÃ i Ä‘áº·t há»‡ thá»‘ng")
        
        with st.expander("â„¹ï¸ ThÃ´ng tin há»‡ thá»‘ng & HÃ nh Ä‘á»™ng", expanded=False):
            loader_mode = "Auto-detection" if os.getenv('USE_UNSTRUCTURED', 'false').lower() == 'true' else "Custom"
            st.info(f"ğŸ“‹ Loader: **{loader_mode}**")
            st.info(f"ğŸ¤– Model: **{os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'N/A')}**")
            st.info(f"ğŸ’¬ Tin nháº¯n: **{len(st.session_state.messages)}**")
            
            st.markdown("---")
            
            if st.button("ğŸ“š Táº£i láº¡i tÃ i liá»‡u", use_container_width=True):
                with st.spinner("Äang táº£i tÃ i liá»‡u..."):
                    documents = load_documents()
                    if documents:
                        st.success(f"âœ… ÄÃ£ táº£i {len(documents)} tÃ i liá»‡u")
                    else:
                        st.info("ğŸ“‚ KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u má»›i")
        
        st.markdown("---")
        st.subheader("ğŸ’¡ HÆ°á»›ng dáº«n sá»­ dá»¥ng")
        st.markdown("""
        **CÃ¡ch sá»­ dá»¥ng:**
        1. ğŸ—£ï¸ MÃ´ táº£ triá»‡u chá»©ng cá»§a báº¡n
        2. ğŸ‘¨â€âš•ï¸ Há»i vá» bÃ¡c sÄ© phÃ¹ há»£p
        3. ğŸ’Š TÆ° váº¥n vá» thuá»‘c Ä‘iá»u trá»‹
        
        **LÆ°u Ã½:**
        - âš ï¸ ThÃ´ng tin chá»‰ mang tÃ­nh tham kháº£o
        - ğŸ¥ LuÃ´n tham kháº£o bÃ¡c sÄ© trÆ°á»›c khi Ä‘iá»u trá»‹
        - ğŸ’Š KhÃ´ng tá»± Ã½ dÃ¹ng thuá»‘c
        """)
    
    # Main content
    st.markdown('<div class="progress-section">', unsafe_allow_html=True)
    st.markdown("#### ğŸ“Š Tiáº¿n trÃ¬nh tÆ° váº¥n")
    display_conversation_step()
    st.markdown('</div>', unsafe_allow_html=True)
    st.markdown("---")
    
    # Chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input
    prompt = st.chat_input("ğŸ’¬ Nháº­p cÃ¢u há»i cá»§a báº¡n...")
    
    if prompt:
        # LÆ°u tin nháº¯n gá»‘c cá»§a user
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        with st.chat_message("assistant"):
            with st.spinner("ğŸ¤” Äang tráº£ lá»i..."):
                try:
                    conversation_context = build_conversation_context(st.session_state.chat_history)
                    
                    # QUAN TRá»ŒNG: Chá»‰ láº¥y lá»‹ch sá»­ TRÆ¯á»šC cÃ¢u há»i hiá»‡n táº¡i
                    # KhÃ´ng bao gá»“m prompt hiá»‡n táº¡i khi check symptoms
                    user_only_context = " ".join(st.session_state.user_messages_only[-6:])
                    
                    # Append sau khi Ä‘Ã£ láº¥y context
                    st.session_state.user_messages_only.append(prompt)
                    
                    routing_result = st.session_state.router.route(
                        prompt, 
                        conversation_context,
                        user_only_context
                    )
                    
                    icon, box_class, intent_name = get_intent_icon_and_color(routing_result["intent"])
                    
                    # Chá»‰ hiá»ƒn thá»‹ info box cho cÃ¡c intent y táº¿ (khÃ´ng hiá»ƒn thá»‹ cho general_chat)
                    if routing_result["intent"] != "general_chat":
                        st.markdown(f"""
                        <div class="info-box {box_class}">
                            <strong>{icon} {intent_name}</strong><br>
                            {'âœ… Sá»­ dá»¥ng dá»¯ liá»‡u y táº¿' if routing_result['use_context'] else 'â„¹ï¸ Tráº£ lá»i tá»•ng quÃ¡t'}
                        </div>
                        """, unsafe_allow_html=True)
                    
                    # Warning cho medicine inquiry
                    if routing_result["intent"] == "medicine_inquiry":
                        st.markdown("""
                        <div class="warning-box">
                            âš ï¸ <strong>LÆ°u Ã½ quan trá»ng:</strong> ThÃ´ng tin thuá»‘c chá»‰ mang tÃ­nh tham kháº£o. 
                            Vui lÃ²ng tham kháº£o bÃ¡c sÄ©/dÆ°á»£c sÄ© trÆ°á»›c khi sá»­ dá»¥ng báº¥t ká»³ loáº¡i thuá»‘c nÃ o.
                        </div>
                        """, unsafe_allow_html=True)
                    
                    prompt_template = ChatPromptTemplate.from_messages([
                        ("system", routing_result.get("system_prompt", "Báº¡n lÃ  trá»£ lÃ½ AI.")),
                        MessagesPlaceholder(variable_name="history"),
                        ("human", "{input}")
                    ])
                    
                    chain = prompt_template | st.session_state.llm
                    conversation = RunnableWithMessageHistory(
                        chain,
                        lambda session_id: st.session_state.chat_history,
                        input_messages_key="input",
                        history_messages_key="history"
                    )
                    
                    full_input = routing_result["prompt"]
                    
                    response_placeholder = st.empty()
                    full_response = ""
                    
                    for chunk in conversation.stream(
                        {"input": full_input},
                        config={"configurable": {"session_id": "default"}}
                    ):
                        if hasattr(chunk, 'content'):
                            full_response += chunk.content
                            response_placeholder.markdown(full_response + "â–Œ")
                    
                    response_placeholder.markdown(full_response)
                    
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": full_response,
                        "metadata": {
                            "intent": routing_result["intent"],
                            "use_context": routing_result["use_context"]
                        }
                    })
                    
                except Exception as e:
                    error_msg = f"âŒ Lá»—i: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})

if __name__ == "__main__":
    main()

