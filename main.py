import os
import sys
from typing import List
from langchain.schema import Document
from dotenv import load_dotenv
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from src.models.llm import get_llm, get_embeddings
from src.services.vector_store import VectorStoreService
from src.utils.document_loader import DocumentLoader
from src.agents.router_graph import AgentRouterGraph  # ‚Üê Thay ƒë·ªïi import


load_dotenv()

class AIWorkshopChatbot:
    """Chatbot s·ª≠ d·ª•ng RAG v·ªõi Azure OpenAI v√† LangChain"""

    def __init__(self):
        """Kh·ªüi t·∫°o chatbot"""
        try:
            required_env_vars = [
                'AZURE_OPENAI_API_KEY',
                'AZURE_OPENAI_ENDPOINT', 
                'AZURE_OPENAI_DEPLOYMENT_NAME'
            ]
            
            missing_vars = [var for var in required_env_vars if not os.getenv(var)]
            if missing_vars:
                raise ValueError(f"Thi·∫øu bi·∫øn m√¥i tr∆∞·ªùng: {', '.join(missing_vars)}")
            
            # ƒê·ªçc c·∫•u h√¨nh use_unstructured t·ª´ .env
            use_unstructured = os.getenv('USE_UNSTRUCTURED', 'false').lower() == 'true'
            
            self.llm = get_llm(streaming=True)
            self.vector_service = VectorStoreService()
            self.document_loader = DocumentLoader(use_unstructured=use_unstructured)
            self.router = AgentRouterGraph(vector_service=self.vector_service)  # ‚Üê LangGraph Router
            self.user_messages_only = []
            
            # S·ª≠ d·ª•ng approach m·ªõi c·ªßa LangChain
            self.chat_history = ChatMessageHistory()
            
            # T·∫°o prompt template v·ªõi history
            self.prompt = ChatPromptTemplate.from_messages([
                ("system", "B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh, th√¢n thi·ªán v√† h·ªØu √≠ch."),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{input}")
            ])
            
            # T·∫°o chain v·ªõi message history
            self.chain = self.prompt | self.llm
            self.conversation = RunnableWithMessageHistory(
                self.chain,
                lambda session_id: self.chat_history,
                input_messages_key="input",
                history_messages_key="history"
            )
            
            self.conversation_history = []
            
            try:
                self.vector_service.load_vector_store()
                print("‚úÖ ƒê√£ load vector store")
            except Exception:
                print("‚ö†Ô∏è Ch∆∞a c√≥ vector store, ƒëang ki·ªÉm tra documents...")
                self.auto_load_documents()
                
        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o: {str(e)}")
            sys.exit(1)

    def auto_load_documents(self):
        """T·ª± ƒë·ªông load v√† t·∫°o vector store t·ª´ documents"""
        folder_path = "./data/documents"
        
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
            print("üìÅ ƒê√£ t·∫°o th∆∞ m·ª•c data/documents")
            return
        
        documents = self.document_loader.load_documents_from_folder(folder_path)
        
        if documents:
            print(f"üìö ƒêang t·∫°o vector store t·ª´ {len(documents)} t√†i li·ªáu...")
            self.vector_service.create_vector_store(documents)
            print("‚úÖ ƒê√£ t·∫°o vector store th√†nh c√¥ng")
        else:
            print("üìÇ Kh√¥ng c√≥ t√†i li·ªáu. Th√™m file v√†o data/documents")

    def load_documents_from_folder(self, folder_path: str = "./data/documents"):
        try:
            if not os.path.exists(folder_path):
                os.makedirs(folder_path)
                return []
                
            documents = self.document_loader.load_documents_from_folder(folder_path)
            
            if documents:
                self.vector_service.create_vector_store(documents)
                print(f"üìö ƒê√£ t·∫£i {len(documents)} t√†i li·ªáu")
                
            return documents
            
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i t√†i li·ªáu: {str(e)}")
            return []

    def get_context_from_query(self, query: str, k: int = 3) -> str:
        try:
            if not self.vector_service.vector_store:
                return ""
                
            relevant_docs = self.vector_service.similarity_search(query, k=k)
            if relevant_docs:
                context = "\n\n".join([doc.page_content for doc in relevant_docs])
                return f"Th√¥ng tin tham kh·∫£o:\n{context}\n\n"
            return ""
            
        except Exception as e:
            if "key_model_access_denied" in str(e):
                print(f"‚ùå L·ªói embedding model: {os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME', 'kh√¥ng t√¨m th·∫•y')}")
            return ""

    def _build_conversation_context(self) -> str:
        """T·∫°o context t·ª´ l·ªãch s·ª≠ h·ªôi tho·∫°i"""
        if not self.chat_history.messages:
            return ""
        
        # L·∫•y 6 tin nh·∫Øn g·∫ßn nh·∫•t (3 c·∫∑p h·ªèi-ƒë√°p)
        recent_messages = self.chat_history.messages[-6:]
        
        context_parts = []
        for msg in recent_messages:
            role = "B·ªánh nh√¢n" if msg.type == "human" else "B√°c sƒ©"
            context_parts.append(f"{role}: {msg.content}")
        
        return "\n".join(context_parts)

    def _build_user_messages_only(self) -> str:
        """T·∫°o context CH·ªà t·ª´ tin nh·∫Øn c·ªßa USER"""
        if not self.chat_history.messages:
            return ""
        
        recent_messages = self.chat_history.messages[-6:]
        user_messages = []
        
        for msg in recent_messages:
            if msg.type == "human":
                user_messages.append(msg.content)
        
        return " ".join(user_messages)

    def chat(self, user_input: str) -> str:
        """X·ª≠ l√Ω chat v·ªõi RAG v√† streaming"""
        try:
            # L∆∞u tin nh·∫Øn user g·ªëc
            self.user_messages_only.append(user_input)
            
            conversation_context = self._build_conversation_context()
            user_only_context = " ".join(self.user_messages_only[-6:])  # 6 tin nh·∫Øn g·∫ßn nh·∫•t
            
            print(f"üîç USER ONLY CONTEXT: '{user_only_context}'")
            
            routing_result = self.router.route(
                user_input, 
                conversation_context,
                user_only_context
            )
            
            # C·∫≠p nh·∫≠t system prompt
            self.prompt = ChatPromptTemplate.from_messages([
                ("system", routing_result.get("system_prompt", "B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh, th√¢n thi·ªán v√† h·ªØu √≠ch.")),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{input}")
            ])
            
            self.chain = self.prompt | self.llm
            self.conversation = RunnableWithMessageHistory(
                self.chain,
                lambda session_id: self.chat_history,
                input_messages_key="input",
                history_messages_key="history"
            )
            
            full_input = routing_result["prompt"]
            
            # Stream response
            full_response = ""
            for chunk in self.conversation.stream(
                {"input": full_input},
                config={"configurable": {"session_id": "default"}}
            ):
                if hasattr(chunk, 'content'):
                    content = chunk.content
                    full_response += content
                    # Print t·ª´ng chunk ra m√†n h√¨nh
                    print(content, end='', flush=True)
            
            print()  # Xu·ªëng d√≤ng sau khi stream xong
            
            self.conversation_history.append({
                "user": user_input,
                "assistant": full_response,
                "intent": routing_result["intent"]
            })
            
            return full_response
            
        except Exception as e:
            return f"L·ªói: {str(e)}"

    def clear_memory(self):
        """X√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i"""
        self.chat_history.clear()
        self.conversation_history = []
        self.user_messages_only = []  # ‚Üê X√≥a c·∫£ user_messages_only
        print("‚úÖ ƒê√£ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i")

    def get_stats(self) -> str:
        doc_count = 0
        if self.vector_service.vector_store:
            try:
                doc_count = len(self.vector_service.vector_store.get()['ids']) if hasattr(self.vector_service.vector_store, 'get') else "N/A"
            except:
                doc_count = "N/A"
                
        return f"""
üìä Th·ªëng k√™:
‚Ä¢ T√†i li·ªáu: {doc_count}
‚Ä¢ Cu·ªôc h·ªôi tho·∫°i: {len(self.conversation_history)}
‚Ä¢ Model: {os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'N/A')}
"""


def print_welcome():
    print("\nü§ñ AI WORKSHOP - RAG CHATBOT")
    print("L·ªánh: /exit, /clear, /help, /load, /stats\n")


def main():
    try:
        print_welcome()
        chatbot = AIWorkshopChatbot()
        chatbot.load_documents_from_folder()

        while True:
            try:
                user_input = input("üë§ B·∫°n: ").strip()
                if not user_input:
                    continue

                if user_input.lower() in ['/exit', '/quit', 'exit', 'quit']:
                    print("üëã T·∫°m bi·ªát!")
                    break
                elif user_input.lower() in ['/clear', 'clear']:
                    os.system('cls' if os.name == 'nt' else 'clear')
                    chatbot.clear_memory()
                    print_welcome()
                    continue
                elif user_input.lower() in ['/help', 'help']:
                    print_welcome()
                    continue
                elif user_input.lower() in ['/reload', 'reload']:
                    print("\nüîÑ ƒêang t·∫£i l·∫°i t√†i li·ªáu...")
                    documents = chatbot.load_documents_from_folder()
                    continue
                elif user_input.lower() in ['/stats', 'stats']:
                    print(chatbot.get_stats())
                    continue

                print("\nü§ñ AI: ", end="", flush=True)
                chatbot.chat(user_input)
                print()

            except KeyboardInterrupt:
                print("\nüëã T·∫°m bi·ªát!")
                break
            except Exception as e:
                print(f"‚ùå L·ªói: {str(e)}")

    except Exception as e:
        print(f"‚ùå L·ªói kh·ªüi ƒë·ªông: {str(e)}")


if __name__ == "__main__":
    main()

